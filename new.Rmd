---
title: "Assignment 10"
author: "Ahmet Hatip, Jed Huang, Eric Xin"
output:
  html_document:
    df_print: paged
---

#Cleaning Up the Data
```{r}
#library(lubridate)
#library(readr)
#library(bestglm)
#library(stringr)
if (!require("pacman")) install.packages("pacman")
pacman::p_load(lubridate, readr, bestglm, stringr)
```

```{r}
spring=read_csv("spring_maple_monitoringAug17clean.csv")
fall=read_csv("fall_maple_monitoringAug17coded.csv")
fall<- data.frame(fall)
spring<- data.frame(spring)
print(dim(spring))
print(dim(fall))
```
```{r}
colnames(fall)
#Dropping code columns in
fall=fall[ , -which(names(fall) %in% c("Concrete.Code","Habitat.Code","Shade.Code","Leaf.Color.Code","Leaf.Drop.Code","Fruit.Code","Maple.Code"))]
#changing names for columns to match
colnames(spring)[which(names(spring) == "Tree.ID.Number")] <- "Tree.ID"
colnames(spring)[which(names(spring) == "Shading")] <- "Shade"
colnames(fall)[which(names(fall) == "Tree.Circumference_in")] <- "Circumference_in"
#deleting duplicated
print(dim(fall))
print(dim(spring))
fall=fall[!(duplicated(fall) | duplicated(fall, fromLast = TRUE)), ]
spring=spring[!(duplicated(spring) | duplicated(spring, fromLast = TRUE)), ]
print(dim(fall))
print(dim(spring))
```

```{r}
# going to combine Srping's Flowers and Fall's Fruit columns
colnames(fall)[which(names(fall) == "Fruit")] <- "Fruit-Flowers"
colnames(spring)[which(names(spring) == "Flowers")] <- "Fruit-Flowers"
fall$Seasons="Fall"
spring$Seasons="Spring"
# making fall and spring combinable
fall$Leaf.buds=""
fall$Leaves.unfolded=""
fall$Leaves.unfolded=""
spring$Damage=""
spring$Leaf.Color=""
spring$Leaves.Dropping=""
spring$Other.Comments=""
fall$Tree.health=""
sort(colnames(fall))
length(colnames(fall))
sort(colnames(spring))
length(colnames(spring))
```
```{r}
#combined them both
full1=rbind(fall, spring)
```

```{r}
#looked at map, Longitude should be between -100 to -40 and Latitude between 35 to 50
useM=subset(full1,(Longitude>-100)&(Longitude<-40)&(Latitude>35)&(Latitude<50))
#looking at hist for numeric values to see if there is any outliers
for(i in c("Circumference_in","Latitude","Longitude")){
  hist(useM[,i])
}
dim(useM)
```

```{r}
#Circumference_in should always be postive and can't be greater than 1500 becauase #https://en.wikipedia.org/wiki/List_of_superlative_trees#Stoutest
full=subset(useM,(Circumference_in>0)&(Circumference_in<1500))
dim(full)
```
```{r}
unique(full$Shade)
```

```{r}
for(i in unique(full$Shade)){
  full$Shade[full$Shade==i]=str_trim(sub("\\(.*","",i))
}
unique(full$Shade)
unique(full$Maple.Species)
```


```{r}
#adding coded values
#scaling Latitude, Longitude
full$LatitudeS=scale(full$Latitude)
full$LongitudeS=scale(full$Longitude)
full$MSF=factor(full$Maple.Species)
full$DateN=strptime(full$Date, "%m/%d/%y")
#getting year-min(year)
full$yearM=factor(as.numeric(year(full$DateN))-min(as.numeric(year(full$DateN))))
full$ConcreteF= ifelse(full$Concrete=="Yes", 1, 0)
full$SF= factor(full$Shade)
full$SeasonsF=factor(full$Seasons)
full$HabitatF=factor(full$Habitat)
#some Circumference_in are negative, will make them positive
full$Circumference_inP=abs(full$Circumference_in)
useMe1=full[,c("Circumference_inP","ConcreteF","yearM","HabitatF","LatitudeS","LongitudeS","MSF","SF","SeasonsF")]
```

```{r}
for(col in colnames(useMe1)){
  cat("\n",col,sum(is.na(useMe1[col])))
}
dim(useMe1)
useMe=na.omit(useMe1)
dim(useMe)
```


# Section 1: ANOVA Analysis
```{r}
#one: tree size and type
oneA=aov(Circumference_inP~MSF,data=useMe)
summary(oneA)
hist(oneA$residuals)
qqnorm(oneA$residuals)
qqline(oneA$residuals)
tapply(useMe$Circumference_inP, useMe$MSF, sd)
plot(oneA$residuals~oneA$fitted.values)
abline(0,0)
#plot(oneA)
TukeyHSD(oneA)
```

Conditions:  
1- Zero mean: The residuals have zero mean because the model is calculated based on the means  
2- Constant variance: No species has a standard deviation that is twice as large as another, so even though the Norway Maple has a very large standard deviation, this condition is still met  
3- Normality: The residuals seem to be pretty normally distributed, although there is a noticeable right skew as shown in the histogram and the QQ plot. This could potentially lower the usefulness of this model  
4- Independence: One of our assumptions, we cannot prove it 

The null hypothesis is that all of the mean circumferences of each type of tree is the same, that is, the mean circumference of a tree is not affected by its species. The alternative hypothesis is that at least two different species have different mean circumferences.

We get a p-value of <2e-16, or essentially zero, so we reject the null hypothesis and accept the alternative hypothesis that at least two different species in the dataset have different mean circumferences. This shows that the species of maples can help explain the Circumference of the maple trees which makes sense.

From the TukeyHSD, we can see that all of the trees' circumference means are significantly different from each other and from the most different to the least is: Norway, Red, Silver, Sugar.

This analysis makes sense because different tree species have different sizes.

```{r}
#two: tree size in type and shade
twoA=aov(Circumference_inP~MSF+SF,data=useMe)
summary(twoA)
hist(twoA$residuals)
qqnorm(twoA$residuals)
qqline(twoA$residuals)
tapply(useMe$Circumference_inP, useMe$SF, sd)
plot(twoA$residuals~twoA$fitted.values)
abline(0,0)
#plot(twoA)
TukeyHSD(twoA)
```

Conditions:  
1- Zero mean: The residuals have a mean of zero, but there is more points with negative residuals as the fitted values increase  
2- Constant variance: The standard deviation of the Open Shading group is more than twice as large as that of the Shaded group, which means that there might not be constant variance between the different groups. We also see this in the residual plot, where some groups of residuals have a much smaller spread than other groups of residuals.  
3- Normality: The distribution of the residuals looks almost identical to that of the first model. The residuals seem to be pretty normally distributed but there is that concern of a positive, right skew.  
4- Independence: One of our assumptions, we cannot prove it  

The null hypotheses are that all of the mean circumferences of each type of tree is the same, that is, the mean circumference of a tree is not affected by its species or shading. The alternative hypotheses are that at least two different species have different mean circumferences and that at least two different groups of shaded trees have different mean circumferences.

With p-values of <2e-16 and 9.37e-12 for Maples Species and Shading, respectively, which are both less than the alpha level of 0.05, we reject the null hypotheses and accept the alternative hypotheses that at least two different species and at least two different groups of shaded trees in our dataset have different mean circumferences.

From the TukeyHSD, we see that all of the trees' means are significantly different. We saw from the last model as well that the most different species to the least it is: Norway, Red, Silver, Sugar
From the different shading groups, the most different circumference means to the least it is: Shaded, Partially Shaded, Open.

This analysis makes sense because the type of tree affects how big it is as well as how much sunlight it receives, which affects how much it can grow.

```{r}
#two with: tree size in type and shading interaction
twoW=aov(Circumference_inP~MSF+SF+MSF*SF,data=useMe)
summary(twoW)
hist(twoW$residuals)
qqnorm(twoW$residuals)
qqline(twoW$residuals)
plot(twoW$residuals~twoW$fitted.values)
abline(0,0)
#plot(twoW)
```

Conditions:  
1- Zero mean: The residuals look centered around zero  
2- Constant variance: We know from the previous model that there might be some issues with the constant variance between the different groups, and this residual plot that includes the interaction term also reflects this  
3- Normality: Again, the residuals look normal but slightly positively skewed  
4- Independence: One of our assumptions, we cannot prove it  

The null hypotheses are that all of the mean circumferences of each type of tree is the same, that is, the mean circumference of a tree is not affected by its species or shading or the interaction between the two. The alternative hypotheses are that at least two different species have different mean circumferences, at least two different groups of shaded trees have different mean circumferences, and at least two different interaction groups have different mean circumferences.

With p-values of <2e-16, 6.49e-12, and 0.000396 for Maple Species, Shading, and their interaction, respectively, we reject all null hypotheses and accept the alternative hypotheses that at least two different species, at least two different groups of shaded trees, and at least two different interaction groups in the dataset have different mean circumferences.

This analysis makes sense because the type of tree and how much shade it receives affects how big it can grow, and it's reasonable to think that specific types of shading affects different species of maple trees differently.

```{r}
interAct=lm(Circumference_inP~MSF+SF+MSF*SF,data=useMe)
summary(interAct)
anova(interAct)
interAct$coefficients
```
In addition, all the individual predictors are significant but for some interaction coefficients like MSFSilver Maple:SFShaded and MSFSugar Maple:SFPartially Shaded it produces NA, probably because there are no trees that is silver Maple and shaded and Sugar and Partially shaded

```{r}
unique(useMe$MSF)
unique(useMe$SF)
dim(useMe)[1]
dim(useMe[(useMe$MSF=="Sugar Maple")&(useMe$SF=="Partially Shaded"),])[1]
dim(useMe[(useMe$MSF=="Silver Maple")&(useMe$SF=="Shaded"),])[1]
```

```{r}
for(i in unique(useMe$MSF)){
  for(j in unique(useMe$SF)){
    cat("\n",i," ",j, dim(useMe[(useMe$MSF==i)&(useMe$SF==j),])[1])
  }
}
```
It could also be that because Norway and Partially Shaded have 0 trees, they make the value 0


# Section 2: Logistic Regression
```{r}
# drop all nan values:
logData=na.omit(useMe)
dim(useMe)
dim(logData)
```


```{r}
#predict concrete
drops <- "ConcreteF"
# y is what we are trying to predict
y=logData$ConcreteF
# X is all the variables that will be used to predict
X=logData[ , !(names(logData) %in% drops)]
# idk but for this package u have to do this
Xy<-as.data.frame(cbind(X,y))
```



```{r}
# and do this for it to work
names(logData)<-c(paste("X",1:length(colnames(X)),sep=""),"y")
# and it works
bestAIC <- bestglm(Xy, IC="AIC",family = binomial)
#bestBIC <- bestglm(Xy, IC="BIC",family = binomial)
#bestEBIC <- bestglm(Xy, IC="BICg",family = binomial)
#bestBICq <- bestglm(Xy, IC="BICq",family = binomial)
```

```{r} 
bestAIC
```

```{r}
besLog=glm(ConcreteF~
Circumference_inP+
HabitatF+
MSF+
SF+
SeasonsF,
data=na.omit(useMe)
)
summary(besLog)
cat("\n","p-value",1-pchisq(besLog$null.deviance-besLog$deviance,besLog$df.null-besLog$df.residual))
besLog$null.deviance-besLog$deviance
```

This logistic regression tries to predict if a tree, given its Circumference, Habitat, Species, Shade amount and the Season the tree was checked out, is near Concrete or not. This analysis makes sense for the most part because variables like Circumference, Habitat, Species and Shade amount can be affected based on if the tree is near Concrete or not. But Seasons do not really cause or affect trees to be close to concrete or not because all individual trees witness winter and summer equally.

The G-statistic is calculated to be 167.7399.

The null hypothesis is that all of the predictors have a coefficient of zero and have no effect on determining the probability of whether a tree is close to concrete or not. The alternative hypothesis is that at least one predictor does not have a coefficient of zero and thus the model is useful for determining the probability of whether a tree is close to concrete or not.

Using a chi square distribution with my G-statistic and 12 degrees of freedom, we calculated the p-value to be about 0, so we reject the null hypothesis and conclude that at least one of the predictors is a useful predictor for determining the probability that a tree is close to Concrete or not and thus deeming this model effective.


# Section 3: Multiple Linear Regression
```{r}
#predict tree size all data
library(leaps)
source("ShowSubsets.R")
bigMod = regsubsets(Circumference_inP~ConcreteF+yearM+HabitatF+LatitudeS+LongitudeS+MSF+SF+SeasonsF,data=useMe,nvmax = 26,nbest = 26,really.big=T)
ss=ShowSubsets(bigMod)
ssCP=ss[order(ss$Cp,decreasing = F),]
ssAR=ssCP[order(ssCP$adjRsq,decreasing = T),]
ssCP
ssAR
```
```{r}
(t(ssCP[1,]))
```
This model is the best one because it has the lowest Cp

```{r}
mod2Lm=lm(Circumference_inP~
ConcreteF+  
yearM+     
HabitatF+      
LatitudeS+
MSF+
SF+
SeasonsF,data=useMe)
summary(mod2Lm)
```
Even though this is the best model out of all of the generated subsets of models, the individual year predictors are not significant at all, as all of them have a p-value of above 0.6. Let's create a new model without them in it. This also makes sense in a practical way because if the trees are already well grown, then the year that the data was collected in shouldn't affect their circumferences.

```{r}
mod2Lm=lm(Circumference_inP~
ConcreteF+  
HabitatF+      
LatitudeS+
MSF+
SF+
SeasonsF,data=useMe)
summary(mod2Lm)
```
The null hypothesis is that none of the predictors are useful for predicting the circumference of a tree and all have a coefficient of zero. The alternative hypothesis is that at least one of the predictors is useful for predicting the circumference of a tree and does not have a coefficient of zero.

As expected, we get a p-value of <2e-16, essentially zero, and so we reject the null hypothesis and conclude that our model is effective at predicting the circumference of a tree.
```{r}
hist(mod2Lm$residuals)
qqnorm(mod2Lm$residuals)
qqline(mod2Lm$residuals)
plot(mod2Lm$residuals~mod2Lm$fitted.values)
abline(0,0)
#plot(mod2Lm)
```


Conditions for linear regression:  
1. Zero means: The residuals look like they are centered at zero  
2. Constant Variance: In the residual plot, we can see a slight fan-shaped pattern in the residuals as the fitted values increase. This should be okay, but we should be careful as we move forward with analysis.  
3. Independence: We deleted duplicated data points because of independence, but this will be our biggest assumption because we did not collect the data  
4. Normality: The distribution of the residuals looks like the one we saw in section 1 with the aov models, where it looks normal for the most part but has that slight right skew  

```{r}
mod2Lm$coefficients
```
This model seems to make sense for trying to calculate the circumference of trees for the most part. Using the coefficients we can see which specific characteristics help influence the growth a large tree.

For example, when ConcreteF is 1, meaning that the tree is located within 100 feet of buildings, concrete, or asphalt, they tend to be larger because the coefficient is positive. However, the less sun the tree gets because of shade the smaller it ends up being, which makes sense because trees need sunlight to grow.

One of the predictors that probably makes less sense is the season. It seems to suggest that trees measured in the spring are larger, which is a bit weird because trees do not get randomly wider in the spring. It may just be the case that more trees are accessible in the spring than in the fall because of snow, so there is a biased recording of larger trees in the spring.


# Section 4: Further Analysis

```{r}
full1=na.omit(full1)
library(ggplot2)
library(ggthemes)
library(raster)
library(rgeos)
states=c( "Maine", "New York", "New Jersey", "Vermont", "Massachusetts", "Rhode Island", "Connecticut", "New Hampshire", "Pennsylvania")
prov=c("Ontario","Quebec","New Brunswick")
usa <- getData("GADM", country= "USA", level=1)
cand <- getData("GADM", country= "CAN", level=1)
#ny=bbox(usa[usa$NAME_1 %in% states,])
canp=fortify(cand[cand$NAME_1 %in% prov,])
ny=fortify(usa[usa$NAME_1 %in% states,])
full1$color="grey"
full1$color[(full1$Maple.Species=="Red Maple")]="red"
full1$color[(full1$Maple.Species=="Silver Maple")]="blue"
full1$color[(full1$Maple.Species=="Sugar Maple")]="yellow"
full1$color[(full1$Maple.Species=="Norway Maple")]="green"
```
```{r}
full3=subset(full1,
             (Longitude>-85)
              &(Latitude>10)
             )
full2=subset(full3,
             (Longitude< - 60)
#             &(Latitude<50)
)
plot(y=full2$Latitude,x=full2$Longitude)
useF=full2
table(useF$Maple.Species)
table(useF$color)
```

```{r}
gg <- ggplot(useF)
gg <- gg + geom_map(data=ny, map=ny, 
                    aes(x=long, y=lat, map_id=id, group=group),
                    fill=NA, color="black")
gg <- gg + geom_map(data=canp, map=canp, 
                    aes(x=long, y=lat, map_id=id, group=group),
                    fill=NA, color="black")
gg <- gg + geom_point(data=useF, aes(x=Longitude, y=Latitude,colour = color), 
                      alpha=1, na.rm=TRUE,size=.5)
gg <- gg + scale_size(range=c(2,7))
gg <- gg + scale_color_identity()
gg <- gg + labs(title= "coordinate data of tree species", 
                x="Longitude", y= "Latitude")
gg <- gg + coord_map()
gg <- gg + theme_map()
gg <- gg + theme(title= element_text(hjust = 0.5, vjust=1, face="bold"))
gg
```

We used the ggplot2 package to plot the locations of different trees by projecting them onto a map of the US and Canada to show which types of trees are located where. We used the latitude and longitude of the trees from the dataset to plot them. Red Maples are in red, Silver Maples in blue, Sugar Maples in yellow, and Norway Maples are in green.

It looks like the trees are spread out a bit, but mostly concentrated in New York, which makes sense because the organization is based out of New York's North Country. Thus, this visual analysis of where the trees are located physically makes sense and can help us understand where the different measurements are taking place. This can be helpful to determine if more or less measurements should be made in certain areas.

